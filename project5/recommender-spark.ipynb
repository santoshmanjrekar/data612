{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project -5 Recommender System in Spark\n",
    "#### Author :Santosh Manjrekar\n",
    "\n",
    "In this assignment I will use spark framework for movie recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Spark framework on your machine and see if it is running properly.I used following tutorial to do that.\n",
    "\n",
    "https://bigdata-madesimple.com/guide-to-install-spark-and-use-pyspark-from-jupyter-in-windows/\n",
    "\n",
    "Let's check if insttalation looks good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building movie recommender system using spark framework. We will use movielens 100k data.\n",
    "First we will run the code locally and later deploy it to databricks in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read ratings CSV\n",
    "ratings_df = spark.read.csv('ratings.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[3395] at javaToPython at <unknown>:0\n",
      "Got 100836 ratings from 610 users on 9724 movies.\n"
     ]
    }
   ],
   "source": [
    "ratings = ratings_df.rdd\n",
    "\n",
    "print(ratings)\n",
    "\n",
    "numRatings = ratings.count()\n",
    "numUsers = ratings.map(lambda r: r[0]).distinct().count()\n",
    "numMovies = ratings.map(lambda r: r[1]).distinct().count()\n",
    "\n",
    "print(\"Got %d ratings from %d users on %d movies.\" % (numRatings, numUsers, numMovies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read movies CSV\n",
    "movies_df = spark.read.csv('movies.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieId|count|\n",
      "+-------+-----+\n",
      "|   1580|  165|\n",
      "|   2366|   25|\n",
      "|   3175|   75|\n",
      "|   1088|   42|\n",
      "|  32460|    4|\n",
      "|  44022|   23|\n",
      "|  96488|    4|\n",
      "|   1238|    9|\n",
      "|   1342|   11|\n",
      "|   1591|   26|\n",
      "|   1645|   51|\n",
      "|   4519|    9|\n",
      "|   2142|   10|\n",
      "|    471|   40|\n",
      "|   3997|   12|\n",
      "|    833|    6|\n",
      "|   3918|    9|\n",
      "|   7982|    4|\n",
      "|   1959|   15|\n",
      "|  68135|   10|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_counts = ratings_df.groupBy(\"movieId\").count()\n",
    "movies_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: double, timestamp: int]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df, validation_df, test_df = ratings_df.randomSplit([.6, .2, .2])\n",
    "# training_RDD = training_RDD.rdd.cache()\n",
    "# validation_for_predict_RDD = validation_RDD.rdd.map(lambda x: (x[0], x[1])).cache()\n",
    "# test_for_predict_RDD = test_RDD.rdd.map(lambda x: (x[0], x[1])).cache()\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the diffrenet parameters which we can tune for the ALS model.\n",
    "seed = 5\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = range(4, 12)\n",
    "errors = []\n",
    "err = 0\n",
    "tolerance = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which rank value will be the most optimized one ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(userId=133, movieId=471, rating=4.0, timestamp=843491793, prediction=3.143522024154663), Row(userId=597, movieId=471, rating=2.0, timestamp=941558175, prediction=3.702383518218994), Row(userId=500, movieId=471, rating=1.0, timestamp=1005528017, prediction=3.3468072414398193), Row(userId=57, movieId=471, rating=3.0, timestamp=969753604, prediction=3.749197244644165), Row(userId=610, movieId=471, rating=4.0, timestamp=1479544381, prediction=3.7941508293151855)]\n",
      "For rank 4 the RMSE is 0.8993863379097063\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(userId=133, movieId=471, rating=4.0, timestamp=843491793, prediction=2.8729283809661865), Row(userId=597, movieId=471, rating=2.0, timestamp=941558175, prediction=4.210461139678955), Row(userId=500, movieId=471, rating=1.0, timestamp=1005528017, prediction=2.8438453674316406), Row(userId=57, movieId=471, rating=3.0, timestamp=969753604, prediction=3.4386770725250244), Row(userId=610, movieId=471, rating=4.0, timestamp=1479544381, prediction=3.736049175262451)]\n",
      "For rank 5 the RMSE is 0.917501656248641\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(userId=133, movieId=471, rating=4.0, timestamp=843491793, prediction=2.8816330432891846), Row(userId=597, movieId=471, rating=2.0, timestamp=941558175, prediction=3.9820516109466553), Row(userId=500, movieId=471, rating=1.0, timestamp=1005528017, prediction=3.3979690074920654), Row(userId=57, movieId=471, rating=3.0, timestamp=969753604, prediction=3.6400585174560547), Row(userId=610, movieId=471, rating=4.0, timestamp=1479544381, prediction=3.4689033031463623)]\n",
      "For rank 6 the RMSE is 0.9039962630596899\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(userId=133, movieId=471, rating=4.0, timestamp=843491793, prediction=3.0603556632995605), Row(userId=597, movieId=471, rating=2.0, timestamp=941558175, prediction=4.047281265258789), Row(userId=500, movieId=471, rating=1.0, timestamp=1005528017, prediction=3.0297350883483887), Row(userId=57, movieId=471, rating=3.0, timestamp=969753604, prediction=3.611546516418457), Row(userId=610, movieId=471, rating=4.0, timestamp=1479544381, prediction=3.5681986808776855)]\n",
      "For rank 7 the RMSE is 0.9067537404537431\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(userId=133, movieId=471, rating=4.0, timestamp=843491793, prediction=3.0669002532958984), Row(userId=597, movieId=471, rating=2.0, timestamp=941558175, prediction=4.1961236000061035), Row(userId=500, movieId=471, rating=1.0, timestamp=1005528017, prediction=2.8633010387420654), Row(userId=57, movieId=471, rating=3.0, timestamp=969753604, prediction=3.1738908290863037), Row(userId=610, movieId=471, rating=4.0, timestamp=1479544381, prediction=3.5449235439300537)]\n",
      "For rank 8 the RMSE is 0.9047487898210824\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(userId=133, movieId=471, rating=4.0, timestamp=843491793, prediction=2.9498729705810547), Row(userId=597, movieId=471, rating=2.0, timestamp=941558175, prediction=4.545050144195557), Row(userId=500, movieId=471, rating=1.0, timestamp=1005528017, prediction=3.3001456260681152), Row(userId=57, movieId=471, rating=3.0, timestamp=969753604, prediction=3.6416678428649902), Row(userId=610, movieId=471, rating=4.0, timestamp=1479544381, prediction=3.5347471237182617)]\n",
      "For rank 9 the RMSE is 0.9043885485796219\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(userId=133, movieId=471, rating=4.0, timestamp=843491793, prediction=2.6866672039031982), Row(userId=597, movieId=471, rating=2.0, timestamp=941558175, prediction=3.978813409805298), Row(userId=500, movieId=471, rating=1.0, timestamp=1005528017, prediction=2.574647903442383), Row(userId=57, movieId=471, rating=3.0, timestamp=969753604, prediction=3.2321853637695312), Row(userId=610, movieId=471, rating=4.0, timestamp=1479544381, prediction=3.652043104171753)]\n",
      "For rank 10 the RMSE is 0.90847096648678\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(userId=133, movieId=471, rating=4.0, timestamp=843491793, prediction=3.2796034812927246), Row(userId=597, movieId=471, rating=2.0, timestamp=941558175, prediction=3.9322054386138916), Row(userId=500, movieId=471, rating=1.0, timestamp=1005528017, prediction=1.8921037912368774), Row(userId=57, movieId=471, rating=3.0, timestamp=969753604, prediction=3.5766146183013916), Row(userId=610, movieId=471, rating=4.0, timestamp=1479544381, prediction=3.5868892669677734)]\n",
      "For rank 11 the RMSE is 0.9099095782441916\n",
      "The best model was trained with rank 4\n"
     ]
    }
   ],
   "source": [
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "\n",
    "for rank in ranks:\n",
    "    als = ALS(maxIter=iterations, regParam=regularization_parameter, rank=rank, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "    model = als.fit(training_df)\n",
    "    predictions = model.transform(validation_df)\n",
    "    print(type(predictions))\n",
    "    predictions.describe()\n",
    "    print(predictions.head(5))\n",
    "    new_predictions = predictions[predictions['prediction'] != np.NaN]\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(new_predictions)\n",
    "    errors.append(rmse)\n",
    "\n",
    "    print('For rank %s the RMSE is %s' % (rank, rmse))\n",
    "    if rmse < min_error:\n",
    "        min_error = rmse\n",
    "        best_rank = rank\n",
    "print('The best model was trained with rank %s' % best_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like rank=4 will be the best value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I deployed above code to databricks.\n",
    "\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6740759789383834/2719118021108212/8218724042301946/latest.html\n",
    "    \n",
    "I can immidiately see the performance improvement while running the code. The code runs much faster in the\n",
    "databricks cluster environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "1. We have built a recommender system using spark framework that uses movielens data to predict movie ratings for users, achieving an RMSE of 0.899.\n",
    "2. The approach used is highly scalable, and can be used with computational clusters using HDFS for much larger data files.\n",
    "3. By using a parallel architecture we can make better use of hardware instead of using a pythonic serial calculation approach. This reduces runtimes for larger calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
